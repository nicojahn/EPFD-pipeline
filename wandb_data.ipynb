{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -q -U pip\n",
    "!{sys.executable} -m pip install -q -U -r requirements.txt\n",
    "!{sys.executable} -m pip install -q -U -r ./EPFD/requirements.txt\n",
    "!{sys.executable} -m pip install -q -e ./EPFD/PyEnsemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "project = \"htcv\"\n",
    "run = wandb.init(project=project, entity=\"nicojahn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload the data to W&B as artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_artifact = wandb.Artifact(\"labels\", type=\"raw_dataset\")\n",
    "label_artifact.add_file(\"data/2018_IEEE_GRSS_DFC_GT_TR.tif\")\n",
    "run.log_artifact(label_artifact)\n",
    "label_artifact.wait()\n",
    "\n",
    "data_artifact = wandb.Artifact(\"predictions\", type=\"raw_dataset\")\n",
    "data_artifact.add_dir(\"data/ensemble/\")\n",
    "run.log_artifact(data_artifact)\n",
    "data_artifact.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the data again and use it as artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "attempt = 60*5\n",
    "\n",
    "label_artifact = None\n",
    "while attempt:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        if label_artifact is None:\n",
    "            label_artifact = run.use_artifact(\"nicojahn/\" + project + \"/labels:latest\", type=\"raw_dataset\")\n",
    "        label_artifact_dir = label_artifact._default_root()\n",
    "        \n",
    "        import os\n",
    "        if not os.path.exists(label_artifact_dir):\n",
    "            label_artifact_dir = label_artifact.download()\n",
    "            break\n",
    "        else:\n",
    "            continue_while = False\n",
    "            for file in label_artifact._list():\n",
    "                if not os.path.exists(label_artifact_dir + \"/\" + file):\n",
    "                    continue_while = True\n",
    "                    break\n",
    "            if continue_while:\n",
    "                attempt -= 1\n",
    "                continue\n",
    "            else:\n",
    "                break\n",
    "    except (ValueError, wandb.CommError) as e:\n",
    "        attempt -= 1\n",
    "assert attempt > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "attempt = 60*15\n",
    "\n",
    "data_artifact = None\n",
    "while attempt:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        if data_artifact is None:\n",
    "            data_artifact = run.use_artifact(\"nicojahn/\" + project + \"/predictions:latest\", type=\"raw_dataset\")\n",
    "        data_artifact_dir = data_artifact._default_root()\n",
    "        \n",
    "        import os\n",
    "        if not os.path.exists(data_artifact_dir):\n",
    "            data_artifact_dir = data_artifact.download()\n",
    "            break\n",
    "        else:\n",
    "            continue_while = False\n",
    "            for file in data_artifact._list():\n",
    "                if not os.path.exists(data_artifact_dir + \"/\" + file):\n",
    "                    continue_while = True\n",
    "                    break\n",
    "            if continue_while:\n",
    "                attempt -= 1\n",
    "                continue\n",
    "            else:\n",
    "                break\n",
    "    except (ValueError, wandb.CommError) as e:\n",
    "        attempt -= 1\n",
    "assert attempt > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the data and transform it into label and prediction maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_up = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils import *\n",
    "from utils import *\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "label_file = [str(p) for p in Path(label_artifact_dir).glob(\"*\")]\n",
    "assert len(label_file) == 1\n",
    "label_file = label_file[0]\n",
    "\n",
    "train_samples = [str(p) for p in Path(data_artifact_dir).glob(\"*png\")]\n",
    "assert len(train_samples) == 1\n",
    "train_samples = train_samples[0]\n",
    "\n",
    "predictions = [str(p) for p in Path(data_artifact_dir).glob(\"*\") if p.is_dir()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = read_image(label_file, scale=False)\n",
    "if not scale_up:\n",
    "    labels = scale_image_down(labels)\n",
    "print(labels.shape)\n",
    "plt.imshow(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_codes = get_color_codes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_samples = read_image(train_samples)\n",
    "print(training_samples.shape)\n",
    "plt.imshow(training_samples)\n",
    "plt.show()\n",
    "\n",
    "if scale_up:\n",
    "    training_samples = scale_image_up(training_samples)\n",
    "    print(training_samples.shape)\n",
    "    plt.imshow(training_samples)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "color_list = [\"#%02x%02x%02x\"%tuple(v[\"rgb\"]) for k,v in color_codes.items()]\n",
    "custom_cmap = ListedColormap(color_list)\n",
    "plt.imshow(labels, vmin=1, vmax=len(custom_cmap.colors), cmap=custom_cmap, interpolation='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "which_training_samples = extract_validation_patch(training_samples)\n",
    "which_training_samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red = np.asarray(np.where(which_training_samples[...,0] == 255))\n",
    "blue = np.asarray(np.where(which_training_samples[...,2] == 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "used = np.zeros(which_training_samples.shape[:2])\n",
    "used[red[0],red[1]] = 1\n",
    "plt.imshow(used, vmin=0, vmax=1, cmap=ListedColormap([\"white\", \"red\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "free = np.zeros(which_training_samples.shape[:2])\n",
    "free[blue[0],blue[1]] = 1\n",
    "plt.imshow(free, vmin=0, vmax=1, cmap=ListedColormap([\"white\", \"blue\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert which_training_samples.reshape(-1,3).shape[0] == labels[red[0], red[1]].shape[0] + labels[blue[0], blue[1]].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pixels available for validation\n",
    "# filter for zeros\n",
    "validation_labels = labels[blue[0], blue[1]]\n",
    "pixels_with_classes = validation_labels!=0\n",
    "validation_labels = validation_labels[pixels_with_classes]\n",
    "\n",
    "assert 0 == np.sum(validation_labels==0)\n",
    "print(validation_labels)\n",
    "print(validation_labels.shape)\n",
    "\n",
    "with open(\"validation_labels\",'wb') as file:\n",
    "    pickle.dump(validation_labels, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pixels used for training\n",
    "red = red[:, labels[red[0], red[1]]!=0]\n",
    "labels[red[0], red[1]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_list = {repr(list(v[\"rgb\"])):k for k,v in color_codes.items()}\n",
    "class_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "validation_predictions = {}\n",
    "for setting in predictions:\n",
    "    for tree in sorted([str(p) for p in Path(setting).glob(\"*png\")]):\n",
    "        key = \"/\".join(tree.split(\"/\")[-2:])\n",
    "        \n",
    "        # map color patches to class patches\n",
    "        color_patch = extract_validation_patch(read_image(tree))[blue[0], blue[1]][pixels_with_classes]\n",
    "        class_patch = np.zeros(color_patch.shape[0], dtype=np.uint8)\n",
    "        for k,v in class_list.items():\n",
    "            class_patch[np.all(color_patch==string_to_numpy(k), axis=1)] = v\n",
    "        assert not np.isin(-1, class_patch)\n",
    "        validation_predictions[key] = class_patch\n",
    "\n",
    "with open(\"validation_predictions\",'wb') as file:\n",
    "    pickle.dump(validation_predictions, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload the produced label and prediction maps again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_artifact = wandb.Artifact(\"validation_labels\", type=\"dataset\")\n",
    "label_artifact.add_file(\"validation_labels\")\n",
    "run.log_artifact(label_artifact)\n",
    "label_artifact.wait()\n",
    "\n",
    "data_artifact = wandb.Artifact(\"validation_predictions\", type=\"dataset\")\n",
    "data_artifact.add_file(\"validation_predictions\")\n",
    "run.log_artifact(data_artifact)\n",
    "data_artifact.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
